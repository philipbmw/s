{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7장 에이다부스트 메타 알고리즘으로 분류 개선하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < 학습 목표 >\n",
    "* 성능 향상을 위해 유사한 분류기 결합하기\n",
    "* 에이다부스트 알고리즘 적용하기\n",
    "* 분류 불균형 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 데이터 집합의 다양한 표본을 사용하는 분류기\n",
    "### 7.1.1 배깅: 임의초 추출한 재표본 데이터로부터 분류기 구축하기\n",
    "### 7.1.2 부스팅\n",
    "\n",
    "## 7.2 훈련: 오류에 초점을 맞춘 분류기 개선\n",
    "\n",
    "## 7.3 의사결정 스텀프로 약한 학습기 생성하기\n",
    "#### <span style=\"color:Blue\"><b>리스팅 7.1 의사결정 스텀프 생성 함수</b></span>\n",
    "\n",
    "## 7.4 전체 에이다부스트 알고리즘 구현하기\n",
    "#### <span style=\"color:Blue\"><b>리스팅 7.2 의사결정 스텀프로 에이다부스트 검사</b></span>\n",
    "\n",
    "## 7.5 검사: 에이다부스트로 분류하기\n",
    "#### <span style=\"color:Blue\"><b>리스팅 7.3 에이다부스트 분류 함수</b></span>\n",
    "\n",
    "## 7.6 예제: 에이다부스트에 복잡한 데이터 집합 적용하기\n",
    "#### <span style=\"color:Blue\"><b>리스팅 7.4 에이다부스트를 위해 데이터를 불러오는 함수</b></span>\n",
    "\n",
    "## 7.7 분류 불균형\n",
    "### 7.7.1 또 다른 성능 측정 방법: 정확도, 재현율 그리고 ROC\n",
    "#### <span style=\"color:Blue\"><b>리스팅 7.5 ROC 플롯과 AUC 계산 함수</b></span>\n",
    "### 7.7.2 비용 함수를 가진 분류기의 의사결정 다루기\n",
    "### 7.7.3 분류 불균형이 있는 데이터를 처리하기 위한 데이터 샘플링\n",
    "## 7.8 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 데이터 집합의 다양한 표본을 사용하는 분류기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 배깅: 임의로 추출한 재표본 데이터로부터 분류기 구축하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 부스팅 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 훈련: 오류에 초점을 맞춘 분류기 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 의사결정 스텀프로 약한 학습기 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def loadSimpData():\n",
    "    datMat = matrix([[ 1. ,  2.1],\n",
    "        [ 2. ,  1.1],\n",
    "        [ 1.3,  1. ],\n",
    "        [ 1. ,  1. ],\n",
    "        [ 2. ,  1. ]])\n",
    "    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return datMat,classLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataMat, classLabels = loadSimpData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리스팅 7.1 의사결정 스텀프 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stumpClassify(dataMatrix,dimen,threshVal,threshIneq):#just classify the data\n",
    "    retArray = ones((shape(dataMatrix)[0],1))\n",
    "    if threshIneq == 'lt':\n",
    "        retArray[dataMatrix[:,dimen] <= threshVal] = -1.0\n",
    "    else:\n",
    "        retArray[dataMatrix[:,dimen] > threshVal] = -1.0\n",
    "    return retArray\n",
    "    \n",
    "\n",
    "def buildStump(dataArr,classLabels,D):\n",
    "    dataMatrix = mat(dataArr); labelMat = mat(classLabels).T\n",
    "    m,n = shape(dataMatrix)\n",
    "    numSteps = 10.0; bestStump = {}; bestClasEst = mat(zeros((m,1)))\n",
    "    minError = inf #init error sum, to +infinity\n",
    "    for i in range(n):#loop over all dimensions\n",
    "        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max();\n",
    "        stepSize = (rangeMax-rangeMin)/numSteps\n",
    "        for j in range(-1,int(numSteps)+1):#loop over all range in current dimension\n",
    "            for inequal in ['lt', 'gt']: #go over less than and greater than\n",
    "                threshVal = (rangeMin + float(j) * stepSize)\n",
    "                predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)#call stump classify with i, j, lessThan\n",
    "                errArr = mat(ones((m,1)))\n",
    "                errArr[predictedVals == labelMat] = 0\n",
    "                weightedError = D.T*errArr  #calc total error multiplied by D\n",
    "                #print \"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError)\n",
    "                if weightedError < minError:\n",
    "                    minError = weightedError\n",
    "                    bestClasEst = predictedVals.copy()\n",
    "                    bestStump['dim'] = i\n",
    "                    bestStump['thresh'] = threshVal\n",
    "                    bestStump['ineq'] = inequal\n",
    "    return bestStump,minError,bestClasEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = mat(ones((5,1))/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dim': 0, 'ineq': 'lt', 'thresh': 1.3}, matrix([[ 0.2]]), array([[-1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.]]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buildStump(dataMat, classLabels, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 전체 에이다부스트 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리스팅 7.2 의사결정 스텀프로 에이다부스트 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaBoostTrainDS(dataArr,classLabels,numIt=40):\n",
    "    weakClassArr = []\n",
    "    m = shape(dataArr)[0]\n",
    "    D = mat(ones((m,1))/m)   #init D to all equal\n",
    "    aggClassEst = mat(zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump,error,classEst = buildStump(dataArr,classLabels,D)#build Stump\n",
    "        print \"D:\",D.T\n",
    "        alpha = float(0.5*log((1.0-error)/max(error,1e-16)))#calc alpha, throw in max(error,eps) to account for error=0\n",
    "        bestStump['alpha'] = alpha  \n",
    "        weakClassArr.append(bestStump)                  #store Stump Params in Array\n",
    "        print \"classEst: \",classEst.T\n",
    "        expon = multiply(-1*alpha*mat(classLabels).T,classEst) #exponent for D calc, getting messy\n",
    "        D = multiply(D,exp(expon))                              #Calc New D for next iteration\n",
    "        D = D/D.sum()\n",
    "        #calc training error of all classifiers, if this is 0 quit for loop early (use break)\n",
    "        aggClassEst += alpha*classEst\n",
    "        print \"aggClassEst: \",aggClassEst.T\n",
    "        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))\n",
    "        errorRate = aggErrors.sum()/m\n",
    "        print \"total error: \",errorRate\n",
    "        if errorRate == 0.0: break\n",
    "    return weakClassArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: dim 0, thresh 0.90, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 0.90, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.00, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.00, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.10, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.10, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.20, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 0, thresh 1.20, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 0, thresh 1.30, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.30, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.40, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.40, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.50, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.50, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.60, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.60, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.70, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.70, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.80, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.80, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 1.90, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 0, thresh 1.90, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 0, thresh 2.00, thresh ineqal: lt, the weighted error is 0.600\n",
      "split: dim 0, thresh 2.00, thresh ineqal: gt, the weighted error is 0.400\n",
      "split: dim 1, thresh 0.89, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 0.89, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.00, thresh ineqal: lt, the weighted error is 0.200\n",
      "split: dim 1, thresh 1.00, thresh ineqal: gt, the weighted error is 0.800\n",
      "split: dim 1, thresh 1.11, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.11, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.22, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.22, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.33, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.33, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.44, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.44, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.55, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.55, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.66, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.66, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.77, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.77, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.88, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.88, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 1.99, thresh ineqal: lt, the weighted error is 0.400\n",
      "split: dim 1, thresh 1.99, thresh ineqal: gt, the weighted error is 0.600\n",
      "split: dim 1, thresh 2.10, thresh ineqal: lt, the weighted error is 0.600\n",
      "split: dim 1, thresh 2.10, thresh ineqal: gt, the weighted error is 0.400\n",
      "D: [[ 0.2  0.2  0.2  0.2  0.2]]\n",
      "classEst:  [[-1.  1. -1. -1.  1.]]\n",
      "aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\n",
      "total error:  0.2\n",
      "split: dim 0, thresh 0.90, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 0, thresh 0.90, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 0, thresh 1.00, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.00, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.10, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.10, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.20, thresh ineqal: lt, the weighted error is 0.625\n",
      "split: dim 0, thresh 1.20, thresh ineqal: gt, the weighted error is 0.375\n",
      "split: dim 0, thresh 1.30, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.30, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.40, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.40, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.50, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.50, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.60, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.60, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.70, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.70, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.80, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.80, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.90, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 0, thresh 1.90, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 0, thresh 2.00, thresh ineqal: lt, the weighted error is 0.750\n",
      "split: dim 0, thresh 2.00, thresh ineqal: gt, the weighted error is 0.250\n",
      "split: dim 1, thresh 0.89, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 0.89, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.00, thresh ineqal: lt, the weighted error is 0.125\n",
      "split: dim 1, thresh 1.00, thresh ineqal: gt, the weighted error is 0.875\n",
      "split: dim 1, thresh 1.11, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.11, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.22, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.22, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.33, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.33, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.44, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.44, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.55, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.55, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.66, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.66, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.77, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.77, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.88, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.88, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 1.99, thresh ineqal: lt, the weighted error is 0.250\n",
      "split: dim 1, thresh 1.99, thresh ineqal: gt, the weighted error is 0.750\n",
      "split: dim 1, thresh 2.10, thresh ineqal: lt, the weighted error is 0.750\n",
      "split: dim 1, thresh 2.10, thresh ineqal: gt, the weighted error is 0.250\n",
      "D: [[ 0.5    0.125  0.125  0.125  0.125]]\n",
      "classEst:  [[ 1.  1. -1. -1. -1.]]\n",
      "aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\n",
      "total error:  0.2\n",
      "split: dim 0, thresh 0.90, thresh ineqal: lt, the weighted error is 0.143\n",
      "split: dim 0, thresh 0.90, thresh ineqal: gt, the weighted error is 0.857\n",
      "split: dim 0, thresh 1.00, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.00, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.10, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.10, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.20, thresh ineqal: lt, the weighted error is 0.357\n",
      "split: dim 0, thresh 1.20, thresh ineqal: gt, the weighted error is 0.643\n",
      "split: dim 0, thresh 1.30, thresh ineqal: lt, the weighted error is 0.286\n",
      "split: dim 0, thresh 1.30, thresh ineqal: gt, the weighted error is 0.714\n",
      "split: dim 0, thresh 1.40, thresh ineqal: lt, the weighted error is 0.286\n",
      "split: dim 0, thresh 1.40, thresh ineqal: gt, the weighted error is 0.714\n",
      "split: dim 0, thresh 1.50, thresh ineqal: lt, the weighted error is 0.286\n",
      "split: dim 0, thresh 1.50, thresh ineqal: gt, the weighted error is 0.714\n",
      "split: dim 0, thresh 1.60, thresh ineqal: lt, the weighted error is 0.286\n",
      "split: dim 0, thresh 1.60, thresh ineqal: gt, the weighted error is 0.714\n",
      "split: dim 0, thresh 1.70, thresh ineqal: lt, the weighted error is 0.286\n",
      "split: dim 0, thresh 1.70, thresh ineqal: gt, the weighted error is 0.714\n",
      "split: dim 0, thresh 1.80, thresh ineqal: lt, the weighted error is 0.286\n",
      "split: dim 0, thresh 1.80, thresh ineqal: gt, the weighted error is 0.714\n",
      "split: dim 0, thresh 1.90, thresh ineqal: lt, the weighted error is 0.286\n",
      "split: dim 0, thresh 1.90, thresh ineqal: gt, the weighted error is 0.714\n",
      "split: dim 0, thresh 2.00, thresh ineqal: lt, the weighted error is 0.857\n",
      "split: dim 0, thresh 2.00, thresh ineqal: gt, the weighted error is 0.143\n",
      "split: dim 1, thresh 0.89, thresh ineqal: lt, the weighted error is 0.143\n",
      "split: dim 1, thresh 0.89, thresh ineqal: gt, the weighted error is 0.857\n",
      "split: dim 1, thresh 1.00, thresh ineqal: lt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.00, thresh ineqal: gt, the weighted error is 0.500\n",
      "split: dim 1, thresh 1.11, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.11, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.22, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.22, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.33, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.33, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.44, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.44, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.55, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.55, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.66, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.66, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.77, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.77, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.88, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.88, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 1.99, thresh ineqal: lt, the weighted error is 0.571\n",
      "split: dim 1, thresh 1.99, thresh ineqal: gt, the weighted error is 0.429\n",
      "split: dim 1, thresh 2.10, thresh ineqal: lt, the weighted error is 0.857\n",
      "split: dim 1, thresh 2.10, thresh ineqal: gt, the weighted error is 0.143\n",
      "D: [[ 0.28571429  0.07142857  0.07142857  0.07142857  0.5       ]]\n",
      "classEst:  [[ 1.  1.  1.  1.  1.]]\n",
      "aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\n",
      "total error:  0.0\n"
     ]
    }
   ],
   "source": [
    "classifierArray = adaBoostTrainDS(dataMat, classLabels, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'alpha': 0.6931471805599453, 'dim': 0, 'ineq': 'lt', 'thresh': 1.3},\n",
       " {'alpha': 0.9729550745276565, 'dim': 1, 'ineq': 'lt', 'thresh': 1.0},\n",
       " {'alpha': 0.8958797346140273,\n",
       "  'dim': 0,\n",
       "  'ineq': 'lt',\n",
       "  'thresh': 0.90000000000000002}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifierArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 검사: 에이다부스트로 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리스팅 7.3 에이다부스트 분류 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaClassify(datToClass,classifierArr):\n",
    "    dataMatrix = mat(datToClass)#do stuff similar to last aggClassEst in adaBoostTrainDS\n",
    "    m = shape(dataMatrix)[0]\n",
    "    aggClassEst = mat(zeros((m,1)))\n",
    "    for i in range(len(classifierArr)):\n",
    "        classEst = stumpClassify(dataMatrix, classifierArr[i]['dim'],\\\n",
    "                                 classifierArr[i]['thresh'],\\\n",
    "                                 classifierArr[i]['ineq'])#call stump classify\n",
    "        aggClassEst += classifierArr[i]['alpha']*classEst\n",
    "        print aggClassEst\n",
    "    return sign(aggClassEst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datArr, labelArr = loadSimpData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error:  0.2\n",
      "total error:  0.2\n",
      "total error:  0.0\n"
     ]
    }
   ],
   "source": [
    "classifierArr = adaBoostTrainDS(datArr, labelArr, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69314718]]\n",
      "[[-1.66610226]]\n",
      "[[-2.56198199]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[-1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaClassify([0,0], classifierArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.69314718]\n",
      " [-0.69314718]]\n",
      "[[ 1.66610226]\n",
      " [-1.66610226]]\n",
      "[[ 2.56198199]\n",
      " [-2.56198199]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.],\n",
       "        [-1.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaClassify([[5,5],[0,0]], classifierArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 예제: 에이다부스트에 복잡한 데이터 집합 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리스팅 7.4 에이다부스트를 위해 데이터를 불러오는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataSet(fileName):      #general function to parse tab -delimited floats\n",
    "    numFeat = len(open(fileName).readline().split('\\t')) #get number of fields \n",
    "    dataMat = []; labelMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr =[]\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat-1):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        dataMat.append(lineArr)\n",
    "        labelMat.append(float(curLine[-1]))\n",
    "    return dataMat,labelMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datArr, labelArr = loadDataSet('horseColicTraining2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error:  0.284280936455\n",
      "total error:  0.284280936455\n",
      "total error:  0.247491638796\n",
      "total error:  0.247491638796\n",
      "total error:  0.254180602007\n",
      "total error:  0.240802675585\n",
      "total error:  0.240802675585\n",
      "total error:  0.220735785953\n",
      "total error:  0.247491638796\n",
      "total error:  0.230769230769\n"
     ]
    }
   ],
   "source": [
    "classifierArray = adaBoostTrainDS(datArr, labelArr, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testArr, testLabelArr = loadDataSet('horseColicTest2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [-0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]\n",
      " [ 0.46166238]]\n",
      "[[ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [-0.14917993]\n",
      " [-0.77414483]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [-0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [ 0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.14917993]\n",
      " [ 0.14917993]\n",
      " [ 0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [-0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.14917993]\n",
      " [-0.14917993]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]\n",
      " [ 0.77414483]]\n",
      "[[ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.1376298 ]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [-0.1376298 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [-0.43598966]\n",
      " [-0.43598966]\n",
      " [ 0.4873351 ]\n",
      " [ 0.4873351 ]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [-0.43598966]\n",
      " [-0.43598966]\n",
      " [-1.06095456]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [-0.43598966]\n",
      " [-1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.43598966]\n",
      " [ 0.4873351 ]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [ 0.4873351 ]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [ 0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [-0.1376298 ]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 1.06095456]\n",
      " [ 0.43598966]\n",
      " [ 0.1376298 ]\n",
      " [ 0.4873351 ]\n",
      " [-1.06095456]\n",
      " [ 1.06095456]\n",
      " [-0.1376298 ]\n",
      " [-0.43598966]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]\n",
      " [ 1.06095456]\n",
      " [ 0.4873351 ]]\n",
      "[[ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.37059985]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [ 0.72030514]\n",
      " [-0.37059985]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.37059985]\n",
      " [-0.20301961]\n",
      " [-0.66895971]\n",
      " [ 0.25436505]\n",
      " [ 0.25436505]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [-0.66895971]\n",
      " [-0.66895971]\n",
      " [-0.82798452]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [-0.66895971]\n",
      " [-1.29392461]\n",
      " [ 1.29392461]\n",
      " [ 0.82798452]\n",
      " [ 1.29392461]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.66895971]\n",
      " [ 0.25436505]\n",
      " [ 0.25436505]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.72030514]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.37059985]\n",
      " [ 0.25436505]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [ 0.37059985]\n",
      " [ 0.09534024]\n",
      " [-0.37059985]\n",
      " [ 0.72030514]\n",
      " [ 1.29392461]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.82798452]\n",
      " [ 0.66895971]\n",
      " [-0.09534024]\n",
      " [ 0.72030514]\n",
      " [-1.29392461]\n",
      " [ 0.82798452]\n",
      " [-0.37059985]\n",
      " [-0.66895971]\n",
      " [ 0.82798452]\n",
      " [ 0.72030514]\n",
      " [ 0.82798452]\n",
      " [ 0.25436505]]\n",
      "[[ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 0.56863831]\n",
      " [-0.47092125]\n",
      " [ 0.62994605]\n",
      " [ 0.91834361]\n",
      " [-0.17256139]\n",
      " [ 0.62994605]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.17256139]\n",
      " [-0.00498115]\n",
      " [-0.47092125]\n",
      " [ 0.05632659]\n",
      " [ 0.45240351]\n",
      " [ 0.45240351]\n",
      " [ 1.02602298]\n",
      " [-0.47092125]\n",
      " [-0.47092125]\n",
      " [-0.62994605]\n",
      " [-0.47092125]\n",
      " [ 1.02602298]\n",
      " [-0.47092125]\n",
      " [-1.09588615]\n",
      " [ 1.49196307]\n",
      " [ 1.02602298]\n",
      " [ 1.49196307]\n",
      " [ 0.45240351]\n",
      " [ 1.02602298]\n",
      " [ 0.45240351]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.47092125]\n",
      " [ 0.05632659]\n",
      " [ 0.05632659]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 0.91834361]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.56863831]\n",
      " [ 0.45240351]\n",
      " [-0.47092125]\n",
      " [ 1.02602298]\n",
      " [ 0.56863831]\n",
      " [ 0.2933787 ]\n",
      " [-0.17256139]\n",
      " [ 0.91834361]\n",
      " [ 1.49196307]\n",
      " [ 1.02602298]\n",
      " [ 0.62994605]\n",
      " [ 1.02602298]\n",
      " [ 0.86699817]\n",
      " [ 0.10269822]\n",
      " [ 0.91834361]\n",
      " [-1.09588615]\n",
      " [ 1.02602298]\n",
      " [-0.17256139]\n",
      " [-0.47092125]\n",
      " [ 1.02602298]\n",
      " [ 0.91834361]\n",
      " [ 1.02602298]\n",
      " [ 0.45240351]]\n",
      "[[ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [ 0.75711718]\n",
      " [-0.65940012]\n",
      " [ 0.44146718]\n",
      " [ 0.72986473]\n",
      " [-0.36104026]\n",
      " [ 0.81842493]\n",
      " [ 0.8375441 ]\n",
      " [-0.36104026]\n",
      " [-0.36104026]\n",
      " [ 0.18349772]\n",
      " [-0.65940012]\n",
      " [ 0.24480546]\n",
      " [ 0.64088239]\n",
      " [ 0.64088239]\n",
      " [ 0.8375441 ]\n",
      " [-0.28244237]\n",
      " [-0.65940012]\n",
      " [-0.44146718]\n",
      " [-0.65940012]\n",
      " [ 0.8375441 ]\n",
      " [-0.65940012]\n",
      " [-1.28436502]\n",
      " [ 1.68044194]\n",
      " [ 1.21450185]\n",
      " [ 1.68044194]\n",
      " [ 0.64088239]\n",
      " [ 1.21450185]\n",
      " [ 0.64088239]\n",
      " [ 1.21450185]\n",
      " [-0.36104026]\n",
      " [-0.28244237]\n",
      " [ 0.24480546]\n",
      " [-0.13215228]\n",
      " [ 0.8375441 ]\n",
      " [ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [ 0.72986473]\n",
      " [ 0.8375441 ]\n",
      " [ 1.21450185]\n",
      " [ 1.21450185]\n",
      " [-0.36104026]\n",
      " [-0.38015944]\n",
      " [ 0.26392464]\n",
      " [-0.65940012]\n",
      " [ 0.8375441 ]\n",
      " [ 0.38015944]\n",
      " [ 0.10489983]\n",
      " [-0.36104026]\n",
      " [ 1.10682248]\n",
      " [ 1.68044194]\n",
      " [ 1.21450185]\n",
      " [ 0.81842493]\n",
      " [ 0.8375441 ]\n",
      " [ 0.6785193 ]\n",
      " [-0.08578066]\n",
      " [ 1.10682248]\n",
      " [-0.90740727]\n",
      " [ 0.8375441 ]\n",
      " [-0.36104026]\n",
      " [-0.28244237]\n",
      " [ 1.21450185]\n",
      " [ 1.10682248]\n",
      " [ 0.8375441 ]\n",
      " [ 0.26392464]]\n",
      "[[ 1.36677554]\n",
      " [ 1.06222816]\n",
      " [ 0.60484349]\n",
      " [-0.81167381]\n",
      " [ 0.28919349]\n",
      " [ 0.88213842]\n",
      " [-0.20876657]\n",
      " [ 0.97069862]\n",
      " [ 0.98981779]\n",
      " [-0.51331395]\n",
      " [-0.20876657]\n",
      " [ 0.03122403]\n",
      " [-0.50712643]\n",
      " [ 0.39707915]\n",
      " [ 0.79315608]\n",
      " [ 0.79315608]\n",
      " [ 0.68527041]\n",
      " [-0.43471606]\n",
      " [-0.81167381]\n",
      " [-0.59374087]\n",
      " [-0.50712643]\n",
      " [ 0.98981779]\n",
      " [-0.50712643]\n",
      " [-1.43663871]\n",
      " [ 1.52816825]\n",
      " [ 1.06222816]\n",
      " [ 1.83271563]\n",
      " [ 0.4886087 ]\n",
      " [ 1.06222816]\n",
      " [ 0.4886087 ]\n",
      " [ 1.36677554]\n",
      " [-0.20876657]\n",
      " [-0.43471606]\n",
      " [ 0.09253177]\n",
      " [-0.28442597]\n",
      " [ 0.68527041]\n",
      " [ 1.06222816]\n",
      " [ 1.06222816]\n",
      " [ 1.06222816]\n",
      " [ 0.88213842]\n",
      " [ 0.98981779]\n",
      " [ 1.36677554]\n",
      " [ 1.06222816]\n",
      " [-0.51331395]\n",
      " [-0.53243313]\n",
      " [ 0.11165095]\n",
      " [-0.50712643]\n",
      " [ 0.68527041]\n",
      " [ 0.22788575]\n",
      " [-0.04737386]\n",
      " [-0.51331395]\n",
      " [ 0.95454879]\n",
      " [ 1.52816825]\n",
      " [ 1.06222816]\n",
      " [ 0.66615124]\n",
      " [ 0.98981779]\n",
      " [ 0.52624561]\n",
      " [-0.23805435]\n",
      " [ 1.25909617]\n",
      " [-1.05968096]\n",
      " [ 0.98981779]\n",
      " [-0.51331395]\n",
      " [-0.43471606]\n",
      " [ 1.06222816]\n",
      " [ 0.95454879]\n",
      " [ 0.68527041]\n",
      " [ 0.11165095]]\n",
      "[[ 1.21166683]\n",
      " [ 1.21733687]\n",
      " [ 0.44973479]\n",
      " [-0.96678252]\n",
      " [ 0.13408478]\n",
      " [ 1.03724713]\n",
      " [-0.36387528]\n",
      " [ 0.81558991]\n",
      " [ 0.83470909]\n",
      " [-0.66842266]\n",
      " [-0.36387528]\n",
      " [-0.12388468]\n",
      " [-0.66223514]\n",
      " [ 0.24197045]\n",
      " [ 0.63804737]\n",
      " [ 0.94826478]\n",
      " [ 0.84037912]\n",
      " [-0.58982477]\n",
      " [-0.96678252]\n",
      " [-0.74884958]\n",
      " [-0.66223514]\n",
      " [ 0.83470909]\n",
      " [-0.66223514]\n",
      " [-1.59174742]\n",
      " [ 1.68327696]\n",
      " [ 0.90711945]\n",
      " [ 1.67760692]\n",
      " [ 0.33349999]\n",
      " [ 1.21733687]\n",
      " [ 0.6437174 ]\n",
      " [ 1.52188425]\n",
      " [-0.36387528]\n",
      " [-0.58982477]\n",
      " [-0.06257693]\n",
      " [-0.43953468]\n",
      " [ 0.84037912]\n",
      " [ 1.21733687]\n",
      " [ 1.21733687]\n",
      " [ 0.90711945]\n",
      " [ 0.72702971]\n",
      " [ 0.83470909]\n",
      " [ 1.21166683]\n",
      " [ 0.90711945]\n",
      " [-0.66842266]\n",
      " [-0.68754184]\n",
      " [-0.04345776]\n",
      " [-0.66223514]\n",
      " [ 0.84037912]\n",
      " [ 0.38299446]\n",
      " [-0.20248257]\n",
      " [-0.66842266]\n",
      " [ 0.79944008]\n",
      " [ 1.37305954]\n",
      " [ 1.21733687]\n",
      " [ 0.82125995]\n",
      " [ 1.1449265 ]\n",
      " [ 0.68135431]\n",
      " [-0.39316305]\n",
      " [ 1.10398746]\n",
      " [-1.21478967]\n",
      " [ 1.1449265 ]\n",
      " [-0.66842266]\n",
      " [-0.58982477]\n",
      " [ 1.21733687]\n",
      " [ 0.79944008]\n",
      " [ 0.53016171]\n",
      " [ 0.26675966]]\n",
      "[[ 1.07630486]\n",
      " [ 1.0819749 ]\n",
      " [ 0.31437281]\n",
      " [-0.83142054]\n",
      " [ 0.26944676]\n",
      " [ 1.1726091 ]\n",
      " [-0.22851331]\n",
      " [ 0.95095188]\n",
      " [ 0.97007106]\n",
      " [-0.53306069]\n",
      " [-0.22851331]\n",
      " [-0.25924665]\n",
      " [-0.52687316]\n",
      " [ 0.37733242]\n",
      " [ 0.77340934]\n",
      " [ 1.08362676]\n",
      " [ 0.9757411 ]\n",
      " [-0.4544628 ]\n",
      " [-0.83142054]\n",
      " [-0.88421155]\n",
      " [-0.52687316]\n",
      " [ 0.97007106]\n",
      " [-0.52687316]\n",
      " [-1.45638544]\n",
      " [ 1.81863893]\n",
      " [ 1.04248143]\n",
      " [ 1.8129689 ]\n",
      " [ 0.46886196]\n",
      " [ 1.35269884]\n",
      " [ 0.50835543]\n",
      " [ 1.65724622]\n",
      " [-0.22851331]\n",
      " [-0.4544628 ]\n",
      " [-0.19793891]\n",
      " [-0.30417271]\n",
      " [ 0.9757411 ]\n",
      " [ 1.35269884]\n",
      " [ 1.35269884]\n",
      " [ 1.04248143]\n",
      " [ 0.86239169]\n",
      " [ 0.97007106]\n",
      " [ 1.34702881]\n",
      " [ 1.04248143]\n",
      " [-0.53306069]\n",
      " [-0.55217986]\n",
      " [ 0.09190421]\n",
      " [-0.52687316]\n",
      " [ 0.9757411 ]\n",
      " [ 0.51835643]\n",
      " [-0.06712059]\n",
      " [-0.53306069]\n",
      " [ 0.93480205]\n",
      " [ 1.50842152]\n",
      " [ 1.35269884]\n",
      " [ 0.68589797]\n",
      " [ 1.28028848]\n",
      " [ 0.81671629]\n",
      " [-0.25780108]\n",
      " [ 1.23934943]\n",
      " [-1.0794277 ]\n",
      " [ 1.28028848]\n",
      " [-0.53306069]\n",
      " [-0.4544628 ]\n",
      " [ 1.35269884]\n",
      " [ 0.93480205]\n",
      " [ 0.66552368]\n",
      " [ 0.40212163]]\n",
      "[[ 0.95108899]\n",
      " [ 1.20719077]\n",
      " [ 0.18915694]\n",
      " [-0.95663642]\n",
      " [ 0.14423088]\n",
      " [ 1.29782498]\n",
      " [-0.10329743]\n",
      " [ 0.82573601]\n",
      " [ 1.09528693]\n",
      " [-0.65827656]\n",
      " [-0.35372918]\n",
      " [-0.38446252]\n",
      " [-0.40165729]\n",
      " [ 0.50254829]\n",
      " [ 0.64819347]\n",
      " [ 1.20884263]\n",
      " [ 0.85052522]\n",
      " [-0.57967867]\n",
      " [-0.70620467]\n",
      " [-0.75899568]\n",
      " [-0.65208904]\n",
      " [ 1.09528693]\n",
      " [-0.40165729]\n",
      " [-1.33116957]\n",
      " [ 1.69342306]\n",
      " [ 1.1676973 ]\n",
      " [ 1.68775303]\n",
      " [ 0.34364609]\n",
      " [ 1.22748297]\n",
      " [ 0.38313956]\n",
      " [ 1.53203035]\n",
      " [-0.35372918]\n",
      " [-0.57967867]\n",
      " [-0.32315478]\n",
      " [-0.17895684]\n",
      " [ 0.85052522]\n",
      " [ 1.22748297]\n",
      " [ 1.22748297]\n",
      " [ 0.91726555]\n",
      " [ 0.98760756]\n",
      " [ 0.84485519]\n",
      " [ 1.47224468]\n",
      " [ 0.91726555]\n",
      " [-0.65827656]\n",
      " [-0.67739574]\n",
      " [ 0.21712009]\n",
      " [-0.40165729]\n",
      " [ 0.85052522]\n",
      " [ 0.39314056]\n",
      " [ 0.05809528]\n",
      " [-0.40784481]\n",
      " [ 0.80958618]\n",
      " [ 1.63363739]\n",
      " [ 1.22748297]\n",
      " [ 0.81111385]\n",
      " [ 1.1550726 ]\n",
      " [ 0.69150041]\n",
      " [-0.38301695]\n",
      " [ 1.11413356]\n",
      " [-1.20464357]\n",
      " [ 1.1550726 ]\n",
      " [-0.40784481]\n",
      " [-0.32924692]\n",
      " [ 1.47791472]\n",
      " [ 0.80958618]\n",
      " [ 0.54030781]\n",
      " [ 0.5273375 ]]\n"
     ]
    }
   ],
   "source": [
    "prediction10 = adaClassify(testArr, classifierArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errArr = mat(ones((67,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errArr[prediction10 != mat(testLabelArr).T].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 분류 불균형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.1 또 다른 성능 측정 방법: 정확도, 재현율 그리고 ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리스팅 7.5 ROC 플롯과 AUC 계산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaBoostTrainDS(dataArr,classLabels,numIt=40):\n",
    "    weakClassArr = []\n",
    "    m = shape(dataArr)[0]\n",
    "    D = mat(ones((m,1))/m)   #init D to all equal\n",
    "    aggClassEst = mat(zeros((m,1)))\n",
    "    for i in range(numIt):\n",
    "        bestStump,error,classEst = buildStump(dataArr,classLabels,D)#build Stump\n",
    "        #print \"D:\",D.T\n",
    "        alpha = float(0.5*log((1.0-error)/max(error,1e-16)))#calc alpha, throw in max(error,eps) to account for error=0\n",
    "        bestStump['alpha'] = alpha  \n",
    "        weakClassArr.append(bestStump)                  #store Stump Params in Array\n",
    "        #print \"classEst: \",classEst.T\n",
    "        expon = multiply(-1*alpha*mat(classLabels).T,classEst) #exponent for D calc, getting messy\n",
    "        D = multiply(D,exp(expon))                              #Calc New D for next iteration\n",
    "        D = D/D.sum()\n",
    "        #calc training error of all classifiers, if this is 0 quit for loop early (use break)\n",
    "        aggClassEst += alpha*classEst\n",
    "        #print \"aggClassEst: \",aggClassEst.T\n",
    "        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,1)))\n",
    "        errorRate = aggErrors.sum()/m\n",
    "        print \"total error: \",errorRate\n",
    "        if errorRate == 0.0: break\n",
    "    return weakClassArr, aggClassEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotROC(predStrengths, classLabels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    cur = (1.0,1.0) #cursor\n",
    "    ySum = 0.0 #variable to calculate AUC\n",
    "    numPosClas = sum(array(classLabels)==1.0)\n",
    "    yStep = 1/float(numPosClas); xStep = 1/float(len(classLabels)-numPosClas)\n",
    "    sortedIndicies = predStrengths.argsort()#get sorted index, it's reverse\n",
    "    fig = plt.figure()\n",
    "    fig.clf()\n",
    "    ax = plt.subplot(111)\n",
    "    #loop through all the values, drawing a line segment at each point\n",
    "    for index in sortedIndicies.tolist()[0]:\n",
    "        if classLabels[index] == 1.0:\n",
    "            delX = 0; delY = yStep;\n",
    "        else:\n",
    "            delX = xStep; delY = 0;\n",
    "            ySum += cur[1]\n",
    "        #draw line from cur to (cur[0]-delX,cur[1]-delY)\n",
    "        ax.plot([cur[0],cur[0]-delX],[cur[1],cur[1]-delY], c='b')\n",
    "        cur = (cur[0]-delX,cur[1]-delY)\n",
    "    ax.plot([0,1],[0,1],'b--')\n",
    "    plt.xlabel('False positive rate'); plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve for AdaBoost horse colic detection system')\n",
    "    ax.axis([0,1,0,1])\n",
    "    plt.show()\n",
    "    print \"the Area Under the Curve is: \",ySum*xStep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datArr, labelArr = loadDataSet('horseColicTraining2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error:  0.284280936455\n",
      "total error:  0.284280936455\n",
      "total error:  0.247491638796\n",
      "total error:  0.247491638796\n",
      "total error:  0.254180602007\n",
      "total error:  0.240802675585\n",
      "total error:  0.240802675585\n",
      "total error:  0.220735785953\n",
      "total error:  0.247491638796\n",
      "total error:  0.230769230769\n"
     ]
    }
   ],
   "source": [
    "classifierArray, aggClassEst = adaBoostTrainDS(datArr, labelArr, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEZCAYAAACNebLAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1eQEYWZmNf1/6Cy3tym+j0IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa7541d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Area Under the Curve is:  0.858296963506\n"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "plotROC(aggClassEst.T, labelArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.2 비용 함수를 가진 분류기의 의사결정 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.3 분류 불균형이 있는 데이터를 처리하기 위한 데이터 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <span style=\"color:Blue\"><b>앙상블 메소드</b></span>는 간단하게 하나의 분류기를 사용하는 것보다 더 좋은 답을 구하기 위해서 다양한 분류기들의 예측을 결합하는 방법\n",
    " - 다양한 분류기를 결합하는 것은 과적합과 같은 단일 분류기의 단점을 공략하는 방법\n",
    " - 분류기들이 서로 큰 차이를 보일 때, 다양한 분류기들을 결합하는 것은 효과적임\n",
    "* 앙상블 메소드의 두 가지 유형 - 배깅 & 부스팅\n",
    " - 배깅: 대체할 데이터 집합을 위해 원본 데이터 집합과 같은 크기의 데이터 집합을 임의로 추출한 예제들로 구축\n",
    " - 부스팅: 하나의 데이터 집합에 순차적으로 서로 다른 여러 가지 분류기를 적용함으로써 단계를 추가하는 배깅의 발상 적용\n",
    "* <font color=\"Blue\"><b>에이다부스트</b></font>는 약한 학습기를 사용\n",
    " - 약한 학습기는 가중치 벡터에 의해 가중치가 부여된 입력 데이터를 분류함\n",
    " - 첫 번째 반복에서는 모든 데이터에 같은 가중치가 부여되지만, 그 다음 반복부터는 이전에 부정확하게 분류된 데이터에 대해 더 높은 가중치가 부여됨\n",
    " - 이처럼 오류에 적응하는 것이 에이다부스트의 장점\n",
    "* 에이다부스트 함수는 어떠한 분류기든 가중치가 부여된 데이터를 처리할 수 있도록 적용할 수 있음\n",
    " - 에이다부스트 알고리즘의 성능은 강력하며, 다른 분류기에 사용하기 어려운 데이터 집합도 빠르게 다룸\n",
    "* 분류 불규형 문제는 긍정적인 예제와 부정적인 예제의 개수가 동일하지 않은 데이터를 가지고 분류기를 훈련할 때 발생\n",
    " - 분류 불균형 문제는 분류가 잘못되었을 때, 긍정적인 예제와 부정적인 예제에 대한 비용이 서로 다를 경우에도 발생하게 됨\n",
    "  - 서로 다른 분류기를 평가하는 방법으로 ROC 곡선을 이용\n",
    "  - 어떤 하나의 분류 항목으로 분류하는 것이 이와 다른 분류 항목으로 분류하는 것보다 더 중요할 경우, 분류기의 성능을 통계적으로 측정하기 위해 정확도와 재현율을 사용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
